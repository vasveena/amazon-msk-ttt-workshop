{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"day1/Part%204/1_Intro/exercise/","title":"Introduction","text":"<p>Serverless services allow us to build applications without having to worry about the underlying infrastructure. This allows developers to avoid provisioning, scaling, and managing resource utilization. In this workshop we will build a serverless data pipeline using Amazon MSK Serverless which enables data to be consumed from many different client applications to accomplish downstream tasks such as dashboards and analytics.</p> <p>Attendees will will learn how to:  1. Start a Serverless MSK Cluster  2. Deploy a Kafka Client Container based Lambda Function  3. Ingest data from various sources into a Kafka Client  4. Consume data for downstream tasks such as Quicksight Dashboards </p> <p></p> <p>This is a 300-level workshop, so some familiarly with using serverless services such as Lambda and Fargate ishelpful. No Apache Kafka experience is necessary. This workshop is intended to be finished in 2 hours.</p>"},{"location":"day1/Part%204/2_Setup/exercise/","title":"Setup MSK Serverless cluster","text":""},{"location":"day1/Part%204/2_Setup/exercise/#msk-serverless-overview","title":"MSK Serverless Overview","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. Kafka provides a mechanism to decouple data processing from the source and target destinations as it's highly scalable and resilient. Kafka is very customizable, based on your workload, which introduces operational overhead.</p> <p>With serverless Kafka the tradeoff is you lose the flexibility of being able to configure the capacity of your cluster while gaining the ability to use Kafka through a single interface that just provides an endpoint for clients.</p> <p>In this module we will stand up a Serverless Apache Kafka Cluster by navigating through the AWS Console.</p>"},{"location":"day1/Part%204/2_Setup/exercise/#create-msk-serverless-cluster","title":"Create MSK Serverless Cluster","text":"<p>Navigate to Amazon MSK console page and click the Create cluster button on the top right.</p> <p></p>"},{"location":"day1/Part%204/2_Setup/exercise/#cluster-settings","title":"Cluster Settings","text":"<p>Choose Custom create to customize cluster networking and other settings. Provide the Cluster name.</p> <p></p> <p>Ensure cluster type is selected as Serverless and click Next button.</p> <p></p>"},{"location":"day1/Part%204/2_Setup/exercise/#networking","title":"Networking","text":"<p>In the VPC Configuration section, click Add subnet button to have cluster spread across 3 Availability Zones</p> <p></p> <p>Select MMVPC from the VPC dropdown. For Subnets, select the zones us-east-2a, us-east-2b, us-east-2c and choose Subnets with name starting PrivateSubnetMSK from dropdown for each zone.</p> <p></p> <p>Choose Custom security groups, remove the default security group and choose security group with name MSK Security Group from dropdown list and click Next button.</p> <p></p>"},{"location":"day1/Part%204/2_Setup/exercise/#security","title":"Security","text":"<p>MSK Serverless defaults to IAM role-based authentication. Click Next to proceed.</p> <p></p>"},{"location":"day1/Part%204/2_Setup/exercise/#metrics-and-tags","title":"Metrics and tags","text":"<p>Leave it to default and click Next to proceed.</p> <p></p>"},{"location":"day1/Part%204/2_Setup/exercise/#review-and-create","title":"Review and create","text":"<p>Review the configuration and click Create cluster button.</p> <p></p> <p>Wait for couple of minutes for cluster to be created and available to use. Wait for cluster status to change from Creating to  Active</p> <p> </p>"},{"location":"day1/Part%204/2_Setup/exercise/#copy-cluster-endpoint","title":"Copy Cluster Endpoint","text":"<p>Click on the View client information button and copy the details of cluster endpoint and save it on notepad for later use.</p> <p> </p>"},{"location":"day1/Part%204/2_Setup/exercise/#create-msk-topics","title":"Create MSK Topics","text":"<p>Now that the cluster is ready and available to use, we need to create Kafka topics to produce and consume data. Please follow instructions below to create Kafka topics needed for this workshop.</p>"},{"location":"day1/Part%204/2_Setup/exercise/#connect-to-kafka-client","title":"Connect to Kafka Client","text":"<p>Navigate to Amazon EC2 consolepage.</p> <p>On the EC2 home page click on Instances(running) link as show in below image.</p> <p></p> <p>On the EC2 instances page select the checkbox for the instance named KafkaClientInstance and click on Connect button on top right as shown in below image.</p> <p></p> <p>On the page Connect to instance, ensure you select the Session Manager tab and click on the Connect button.</p> <p></p> <p>This would open a new tab with EC2 terminal.</p>"},{"location":"day1/Part%204/2_Setup/exercise/#create-topics","title":"Create Topics","text":"<p>On the Terminal, please execute below command to change to ec2-user by running the below command:</p> <pre><code>sudo su - ec2-user\n</code></pre> <p></p> <p>Please execute below command to set your MSK cluster broker endpoint to the shell variable BS. Please replaceYOUR_CLUSTER_ENDPOINT in below command with the endpoint you copied in the last step of previous section as shown in the image.</p> <pre><code>export BS=YOUR_CLUSTER_ENDPOINT\n</code></pre> <p></p> <p>Execute command below to create the topics needed for workshop as shown in image</p> <pre><code>./create-topics.sh\n</code></pre> <p>You will see warnings printed to the terminal like below. You may ignore them.</p> <pre><code>SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n</code></pre> <p></p> <p>You should see the four MSK topics (clickstream, Departments_Agg, ClickEvents_UserId_Agg_Result, User_Sessions_Aggregates_With_Order_Checkout) created.</p> <p></p>"},{"location":"day1/Part%204/3_Producer/exercise/","title":"Produce clickstream data to MSK Serverless using Elastic Container Service","text":"<p>Now that we have successfully created topics in our cluster, our next step is to produce data to the cluster. In this step we will deploy a serverlessECS Fargate container that runs an application to produce sample clickstream data to MSK Serverless cluster.</p> <p>Navigate to Amazon ECS console page.</p> <p>On the left side menu click on Task Definitions to navigate to page that displays available Task Definitions.</p> <p> </p> <p>Select the checkbox against the available Task Definition and Click on Run Task option from Deploy menu.</p> <p></p>"},{"location":"day1/Part%204/3_Producer/exercise/#run-task","title":"Run Task","text":"<p>On the Run Task page, select Fargate for Launch type option and for theOperating system family select Linux from drop down. Refer to below image for more details on other fields</p> <p></p> <p>Expand the Networking section.</p> <p></p>"},{"location":"day1/Part%204/3_Producer/exercise/#vpc-and-security-groups","title":"VPC and security groups","text":"<p>Change VPC to MMVPC. Select PrivateSubnetMSKOne, PrivateSubnetMSKTwo and PrivateSubnetMSKThree.</p> <p>For Security Groups, select Use an existing security group, uncheck thedefault security group, and select the security group name starting with msk-serverless-workshop-cfn-ProducerECSTaskSecurity.</p> <p></p>"},{"location":"day1/Part%204/3_Producer/exercise/#advanced-options","title":"Advanced Options","text":"<p>Expand the section under Container Overrides.</p> <p>For BOOTSTRAP_STRING enter the value of your MSK Serverless cluster broker endpoint (you copied this earlier from the Amazon MSK cluster console page, with View client information).</p> <p></p> <p>Scroll to the bottom of the page and click on the Create button.</p> <p></p> <p>On the new page, wait for your task to get into Running status as shown in below image.</p> <p></p> <p>You have now successfully setup a producer ECS task that will continuously keep on producing clickstream data to Kafka topic.</p>"},{"location":"day1/Part%204/3_Producer/exercise/#check-schema-in-aws-glue-schema-registry","title":"Check schema in AWS Glue Schema Registry","text":"<p>Now that the producer ECS task is running, it would create the clickstream schema in AWS Glue Schema Registry.</p> <p>Navigate to Amazon Glue console page and select Stream schema registries from left menu.</p> <p>You would see the schema registry name serverless. Click on the registry name.</p> <p></p> <p>This would show the schema clickstream within the registry. Click on the schema name to see the schema versions. You would see the v1 over there.</p> <p></p> <p>Click on the version v1, this would show you the avro schema of the clickstream data produced by the ECS task.</p> <p></p>"},{"location":"day1/Part%204/4_LambdaConsumer/exercise/","title":"Consume data from MSK Serverless using Lambda","text":"<p>Now that we have setup our MSK Serverless cluster we want to store the datain Amazon S3. There are various options to do that, but in this lab, we will use AWS Lambda.</p> <p>The AWS Lambda function is already deployed for you in your AWS account. The only thing we need to setup is the Trigger. This Functions convert the records from Avro to CSV.</p> <p>Navigate to the AWS Lambda Console and choose click on the MskLambda-Function.</p> <p></p>"},{"location":"day1/Part%204/4_LambdaConsumer/exercise/#setup-amazon-msk-trigger","title":"Setup Amazon MSK Trigger","text":"<p>Click on the Add trigger button to create a new trigger for the Lambda function.</p> <p> </p> <p>Choose MSK as Source. Select your cluster in the drop down menu. Set the configuration parameters for the trigger:</p> <pre><code>- Batch Size: 20\n- Starting Position: Latest\n- Batch window: 20\n- Topic name: clickstream\n</code></pre> <p>After setting the configuration click Add to create the trigger.</p> <p></p> <p>Wait for the trigger to become active (this can take a few minutes).</p> <p></p> <p></p> <p>You have now successfully setup a Lambda functions that reads messages from a Kafka topic, transforms it into csv format and then stores it in Amazon S3.</p>"},{"location":"day1/Part%204/5_QS_Dashboard/exercise/","title":"Create Quicksight Dashboard","text":"<p>Now that we have transformed data, we can now use Amazon QuickSight to visualize and analyze data.</p>"},{"location":"day1/Part%204/5_QS_Dashboard/exercise/#create-a-quicksight-account","title":"Create a QuickSight Account","text":"<p>To begin head to the QuickSight page and create a new standard QuickSight account.</p> <p></p> <p></p> <p>Create a unique QuickSight account user name (e.g workshopuserRANDOM_NUMBERS) and you can use a fake email address for notifications.</p> <p></p> <p>You must also select your s3 bucket with the clickstream data</p> <p></p> <p></p> <p>Once s3 bucket selected, you can finish and create your QuickSight Account</p> <p></p> <p></p>"},{"location":"day1/Part%204/5_QS_Dashboard/exercise/#creating-a-dataset","title":"Creating a Dataset","text":"<p>Once your account is setup head over to the QuickSight Datasets Console. In the Dataset console click New dataset on the top right to create our dataset. Select S3 and enter a name for your dataset.</p> <p>Additionally, we have to create a manifest file to specify the exact data we want to utilize for our analysis. Open a text editor and create a JSON file (e.g. quicksight_manifest.json) and enter the following. Make sure to replace YOUR_DEST_BUCKET with your destination bucket.</p> <p>You may find the name of your destination bucket by navigating to the Amazon S3 Console Page. Select your bucket, copy the name from the bucket detail page, andpaste into your manifest file.</p> <p></p> <pre><code>{\n    \"fileLocations\": [\n        {\n            \"URIPrefixes\": [\n                \"s3://YOUR_DEST_BUCKET/\"\n            ]\n        }\n    ]\n}\n</code></pre> <p></p> <p>Click connect once file is uploaded, then press Visualize to get to the QuickSight analysis creator. You will be presented with the following pop-up. Select Interactive Sheet.</p> <p></p> <p></p>"},{"location":"day1/Part%204/5_QS_Dashboard/exercise/#playtime","title":"Playtime","text":"<p>Now its Playtime, what type of graphs can you create with the data provided?</p> <p>Can you create a pie chart of the data by device type? How about a bar chart highlighting records by event type? *What about a Metric that just has the total number of Events?</p> <p></p> <p>In the top right corner, you can publish your analysis as a Dashboard or export to a PDF. Feel free to check out what others around you have built.</p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/","title":"Consume using Kinesis Data Analytics and write to Amazon OpenSearch Service","text":"<p>In this module, we would like to consume data from MSK Serverless cluster using Amazon Kinesis Data Analytics - Flink, process the data and write the data in Amazon OpenSearch Service</p> <p>The OpenSearch Service is deployed in your AWS account and Dashboards are already configured. We need to configure Kinesis Data Analytics application with correct runtime parameters.</p> <p>Navigate to the AWS Kinesis Analytics Console and click on the application KDAFlinkClickstream-msk-serverless-workshop-cfn.</p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/#configure-kinesis-analytics-application","title":"Configure Kinesis Analytics Application","text":"<p>Click on the Configure button to update the Kinesis Data Analytics configuration.</p> <p></p> <p>Scroll to RunTime Properties section. Update BootStrapServers and keep restof the value as default.</p> <p></p> <p>Save your changes.</p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/#run-kinesis-analytics-application","title":"Run Kinesis Analytics Application","text":"<p>Click on the Run button to run the application. This takes a few minutes.</p> <p></p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/#open-apache-flink-dashboard","title":"Open Apache Flink Dashboard","text":"<p>Once the Kinesis Analytics application is running, click on Open Apache Flink dashboard to open the Flink dashboard.</p> <p></p> <p>Click on the Job Name Flink Streaming Job</p> <p></p> <p>This would show the Job details including the operator directed acyclic graph and data flowing through the operators.</p> <p></p> <p>You have now successfully setup a Kinesis Analytics application that reads messages from a Kafka topic, process the data and then write it to Amazon OpenSearch Service.</p> <p>Lets check the data in OpenSearch Dashboard!</p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/#amazon-opensearch-dashboard-visualisation","title":"Amazon OpenSearch Dashboard Visualisation","text":"<p>In this module, we would see the dashboard visualisation generated based onthe ingested data from Kinesis Analytics application.</p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/#amazon-opensearch-service","title":"Amazon OpenSearch Service","text":"<p>Go to AWS CloudFormation Console and click on stack named msk-serverless-workshop-cfn. Go to the Outputs tab of the stack.</p> <p></p> <p>Right-click on the OpenSearchDashboardEndpoint and click Open Link in new tab. This would try to redirect you to the OpenSearch Dashboard login page.</p> <p>As OpenSearch Service is deployed in a VPC, we are using NGinx reverse proxy to access the OpenSearch Dashboard outside of the VPC.</p> <pre><code>We are using self signed certificate for the Nginx. however recommended to use valid certificate for production.\n</code></pre> <p>If you are accessing the url using Google Chrome, you have to click on the webpage body and type thisisunsafe. This would redirect you to actual OpenSearch login page.</p> <p></p> <p>Go back to the CloudFormation output, retrieve the values for OpenSearchMasterUserName and OpenSearchMasterPassword and use it for login.</p> <pre><code>Ensure that there are no spaces copied for OpenSearchMasterPassword, else you won't be able to login to OpenSearch Dashboard.\n</code></pre> <p>Select Global tenant on the popup dialog box.</p>"},{"location":"day1/Part%204/6_KDA_Consumer/exercise/#amazon-opensearch-dashboard","title":"Amazon OpenSearch Dashboard","text":"<p>Click on the hamburger menu on the left of the screen as highlighted below and then click on Dashboard.</p> <p></p> <p>This would open the OpenSearch Dashboard where you can see the data gets plotted.</p> <p></p> <p>You have now confirmed data flowing to OpenSearch Service and visualizations are rendered.</p>"}]}
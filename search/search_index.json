{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Amazon MSK Train-The-Trainer Workshop","text":"<p>This workshop contains exercises for the 2-day Amazon MSK Workshop Training.</p>"},{"location":"#exercises","title":"Exercises","text":""},{"location":"#day-1","title":"Day-1","text":""},{"location":"#overview","title":"Overview","text":"<ol> <li>Custom configuration</li> <li>Cluster</li> <li>Architecture </li> </ol>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ol> <li>Choose Region</li> <li>Create an MSK security group</li> </ol>"},{"location":"#part-1-kafka-overview-amazon-msk-cluster-creation","title":"Part 1 - Kafka overview &amp; Amazon MSK Cluster Creation","text":"<ol> <li>Launching MSK cluster (Provisoned) (Console)</li> <li>Launching MSK cluster (Provisoned) (CLI)</li> </ol>"},{"location":"#day-2","title":"Day-2","text":""},{"location":"#part-4-building-a-serverless-apache-kafka-data-pipeline","title":"Part 4 - Building a Serverless Apache Kafka Data Pipeline","text":"<ol> <li>Architecture Introduction</li> <li>Setup MSK Serverless cluster</li> <li>Produce clickstream data to MSK Serverless using Elastic Container Service</li> <li>Consume data from MSK Serverless using Lambda</li> <li>Create Quicksight Dashboard</li> <li>Consume using Kinesis Data Analytics and write to </li> </ol>"},{"location":"#part-5-msk-integration-with-athena-and-redshift","title":"Part 5 - MSK Integration with Athena and Redshift","text":"<ol> <li>Architecture Introduction</li> <li>Prerequisites</li> <li>Integration with Athena </li> <li>Integration with Redshift</li> </ol>"},{"location":"setup/","title":"Setup","text":"<p>Perform the following steps to login to the event engine.</p> <ol> <li>Type Event Engine URL on to your browser. (Right click this link -&gt; Open in new tab).</li> <li>Enter the hash provided to you. Accept Terms &amp; Login. </li> <li>Choose \u201cEmail One-Time Password\u201d. Provide your email ID where your 9-digit OTP will be sent within 5 mins. </li> <li>Once you receive OTP over your email, enter it to sign in to the Team Dashboard.  </li> <li>Click on the SSH Key and download the key to your local desktop. Click Ok once done. </li> <li>Click on AWS Console and Open AWS Console. You can also retrieve AWS_DEFAULT_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN from this Team Dashboard whenever required for your exercises. </li> <li>Make a note of your account ID in the top right corner of the AWS Console and store it in a notepad. </li> <li>Go to CloudFormation and verify that the Cloudformation templates \"msk-labs-default\", \"msk-labs-default-MSKVPCStack-xxxxxxxxx\" and \"aws-cloud9-msk-labs-default-Cloud9EC2Bastion-xxxxxxxx\" are created.</li> </ol> <p></p>"},{"location":"day1/part1/1_provisoned/exercise/","title":"Launching an MSK cluster (Console)","text":"<p>This exercise is meant to show you different options and features available while trying to create an MSK cluster. MSK clusters required are already created in your event engine accounts which we will use for our exercises.</p>"},{"location":"day1/part1/1_provisoned/exercise/#create-msk-cluster-console","title":"Create MSK Cluster (Console)","text":"<p>Go to the MSK Landing Page (Right click -&gt; Open Link in New Tab).</p>"},{"location":"day1/part1/1_provisoned/exercise/#cluster-settings","title":"Cluster Settings","text":"<ol> <li>Sign-in to the AWS Console in the account you want to create your cluster in</li> <li>Browse to the MSK create cluster wizard  to start the creation</li> <li>Select Custom create for the Creation method</li> <li>Enter the cluster name - MSKWorkshopCluster</li> <li>In Cluster type, Choose the option Provisioned</li> <li>Select the version of Kafka you want to run on the cluster (ex: 2.8.1) </li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#brokers","title":"Brokers","text":"<ol> <li>Select kafka.m5.large as the Broker Instance Type</li> <li>Select the number of zones (availability zones) you want to deploy in to 3</li> <li>Enter 1 for the number of brokers per zone</li> </ol> <p>(Your cluster will have 3 total brokers, distributed evenly across your 3 Availability Zones.)</p> <p></p>"},{"location":"day1/part1/1_provisoned/exercise/#storage","title":"Storage","text":"<ol> <li>Enter 100 GiB</li> <li>Leave others options as default</li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#configuration-section","title":"Configuration Section","text":"<p>We are going to create a custom configuration that the cluster will run. We will enable the following features:</p> <p>auto.create.topics.enable - allow topics to be created automatically by producers and consumers. This is not typically enabled in a production cluster, but it is handy for development and testing to lower the operational overhead</p> <p>delete.topic.enable - enables topic deletion on the server. If topic deletion is not enabled, you cannot delete topics. You likely want to turn this on on all clusters you build unless you have a specific need not to.</p> <p>log.retention.hours - we will set this to 8 hours for the lab. Note that this is the default configuration, it can still be overridden at the topic level </p>"},{"location":"day1/part1/1_provisoned/exercise/#create-configuration-object","title":"Create Configuration Object","text":"<ol> <li>Select Custom configuration for the Cluster configuration</li> <li>Click on Create configuration  </li> <li>Give your configuration a name - MSK-workshop</li> <li>Add a description for the config - MSK workshop - Auto topic creation; topic deletion; 8hrs retention</li> <li>Under Configuration Properties, ensure the following options are set/overridden - you can leave the Version as None <p>auto.create.topics.enable=true</p> <p>delete.topic.enable=true</p> <p>log.retention.hours=8</p> </li> </ol> <p> 6. Click Create - this will take you to the Cluster Configurations page. You can close this browser tab now   7. Back in the Cluster Creation workflow, you can hit the refresh icon beside the cluster configuration box (the circle icon) then select the configuration you created  </p>"},{"location":"day1/part1/1_provisoned/exercise/#networking","title":"Networking","text":"<ol> <li>Select the VPC you want to deploy your cluster in (MSKVPC if you created the VPC using our provided CloudFormation)</li> <li>Select us-east-1a for the first Availability Zone (AZ), then the subnet (PrivateSubnetMSKOne)</li> <li>Select us-east-1b for the second AZ, and the appropriate subnet (PrivateSubnetMSKTwo)</li> <li>Select us-east-1c for the third AZ, and the appropriate subnet (PrivateSubnetMSKThree) </li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#security-group","title":"Security Group","text":"<ol> <li> <p>In the Security groups in Amazon EC2 section, click Browse and select the msk-labs-default-MSKSecurityGroup-xxxx security group we created in prerequisites steps. </p> </li> <li> <p>Click Choose </p> </li> </ol> <p></p>"},{"location":"day1/part1/1_provisoned/exercise/#security","title":"Security","text":""},{"location":"day1/part1/1_provisoned/exercise/#access-control-methods-authentication","title":"Access control methods (Authentication)","text":"<ol> <li>Select Unauthenticated access for Access control methods. Leave the other access control methods blank as they will be explored in other labs. </li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#encryption","title":"Encryption","text":"<p>Note: You cannot enable encryption on an already created cluster, nor can you turn it off on a cluster configured with encryption, so plan your use carefully to avoid rebuilding to change these settings</p> <ol> <li> <p>In the Encryption section, select both TLS encryption and Plaintext for encryption between clients and brokers. This will enable 2 different service ports on the cluster (9092 and 9094). You will be able to communicate in both an encrypted and unencrypted manner - choose based on your data needs. For this workshop we will experiment with both, but you should choose what fits your production environment best.</p> </li> <li> <p>Select TLS encryption for encryption within the cluster</p> <p>Note: that this can impact the performance  of the cluster in production. If you don't need this level of encryption consider leaving it off.</p> </li> <li> <p>Select Use AWS managed key. This means Amazon MSK will manage the encryption key for you. </p> </li> <li> <p>Click Next</p> </li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#monitoring","title":"Monitoring","text":"<p>There are 2 types of monitoring available for Amazon MSK - Cloudwatch monitoring which is available in 4 flavours (Basic, Enhanced broker-level, Enhanced topic-level, and Enhanced partition-level), as well as the Open Monitoring with Prometheus. We will use both.</p> <ol> <li> <p>Select Enhanced topic-level monitoring. This will enable collection of metrics from each broker at the topic level. This generates more metrics and incurs additional costs, but will also let you troubleshoot and understand your traffic better.</p> </li> <li> <p>Select Enable open monitoring with Prometheus</p> </li> </ol> <p></p>"},{"location":"day1/part1/1_provisoned/exercise/#broker-log-delivery","title":"Broker Log Delivery","text":"<p>We are going to configure the cluster to send any broker logs to Cloudwatch Logs, which will provide us with an easy way to work with the logs.</p> <ol> <li>Click Deliver to Amazon CloudWatch Logs - this will expand a new section where you will enter the ARN for the log group. To do this we will need to create a log group first.</li> <li>Click on visit Amazon CloudWatch Logs console - this will open a new tab in the CloudWatch Logs console </li> <li>Click on Create Log Group in the top right corner </li> <li>Enter your log group name - MSKClusterLogs and click Create </li> <li>You can now close this tab </li> <li>You should be back in your MSK Cluster Creation window - click Browse button, and then check the circle beside MSKClusterLogs and then click Choose </li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#cluster-tags","title":"Cluster tags","text":"<ol> <li>Under key enter Name and in value MSKLabCluster.</li> <li>Click Next</li> </ol>"},{"location":"day1/part1/1_provisoned/exercise/#review-and-create","title":"Review and create","text":"<p>Scroll to the bottom and click Create cluster - voila! Your cluster is being built. This can take 10-15 minutes, so it's a good time to grab a coffee, read ahead in the lab, or explore the MSK Documentation </p> <p></p>"},{"location":"day1/part1/2_cli/exercise/","title":"Launching an MSK cluster (CLI)","text":"<p>In this exercise you will create an Amazon MSK cluster using the AWS CLI.</p>"},{"location":"day1/part1/2_cli/exercise/#prepare-the-enviroment","title":"Prepare the Enviroment","text":""},{"location":"day1/part1/2_cli/exercise/#setting-up-your-cloud9-workstation-environment","title":"Setting Up Your Cloud9 Workstation Environment","text":"<p>AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser while providing you with access to a full Linux server.</p> <p>You will use AWS Cloud9 as your workstation enviroment to run shell commands, Python scripts and SQL queries for the labs.</p>"},{"location":"day1/part1/2_cli/exercise/#accessing-aws-cloud9-terminal-ide","title":"Accessing AWS Cloud9 Terminal IDE","text":"<ol> <li>Go to the AWS Cloud9 Console  and you should see a Cloud9 Environment named msk-labs-default-Cloud9EC2Bastion. (Make sure you are in the right AWS Region).</li> <li>Click on Open to launch your AWS Cloud9 Terminal IDE. A new tab will open in your Browser! <p>Tip: Now, you can close the first AWS Cloud9 tab in your browser and stick to the new AWS Cloud9 Terminal IDE tab that just opened (the one that partially says msk-labs-default-Cloud9EC2Bastion).</p> </li> <li>Allow for the AWS Cloud9 IDE quick setup to finish, then close the Welcome tab</li> <li>To run commands in Cloud9 you need a terminal. Click on the green plus icon (+) and select new terminal to add a new Command Line Terminal tab:  <p>From now on, you will use this terminal window throughout this workshop lab to execute all the AWS CLI commands and scripts.</p> </li> </ol>"},{"location":"day1/part1/2_cli/exercise/#create-msk-cluster-cli","title":"Create MSK Cluster (CLI)","text":""},{"location":"day1/part1/2_cli/exercise/#step-1-get-subnet-information","title":"Step-1: Get Subnet Information","text":"<p>We need to get the subnets to deploy the brokers in to. For that, we need to know the VPC ID for the lab.</p> <ol> <li>Use the cli to get a list of VPCs in your account  <p>aws ec2 describe-vpcs --output table </p> </li> <li>Look in the table for the VPC you're using for the exercise.  If you are using the VPC created as part of the workshop event, it will be named MSKVPC. </li> <li>Copy the VPCid (example: vpc-0eb566c1948800933) to your notepad </li> <li>Use the cli to get a list of subnets in that VPC <p>aws ec2 describe-subnets --filters \"Name=vpc-id,Values=vpc-001ed0757fbb9e2b5\" --output table | egrep \"Name|AvailabilityZone|SubnetId\"</p> </li> </ol> <p>This will list the subnets in the selected VPC, then grab only the AZ, SubnetID, and Name, making it easier for you to grab the SubnetIds for the 3 private subnets in 3 different AZs. Add these to your notepad for later use. Example: (Highlighted in green) </p>"},{"location":"day1/part1/2_cli/exercise/#step-2-create-a-custom-cluster-configuration","title":"Step-2: Create a custom cluster configuration","text":"<p>We are going to enable your new Amazon MSK cluster to have the following settings:</p> Configuration Name Details auto.create.topics.enable allow topics to be created automatically by producers and consumers. This is not typically enabled in a production cluster, but it is handy for development and testing to lower the operational overhead delete.topic.enable enables topic deletion on the server. If topic deletion is not enabled, you cannot delete topics. You likely want to turn this on on all clusters you build unless you have a specific need not to. log.retention.hours we will set this to 8 hours for the lab. Note that this is the default configuration, it can still be overridden at the topic level <ol> <li> <p>On your Cloud9 terminal, create a file called 'cluster_config.txt' with the following command:</p> <p>vi ~/cluster_config.txt</p> </li> <li> <p>Put in the following content (hit i to enter insert mode):</p> <p>auto.create.topics.enable = true</p> <p>delete.topic.enable = true</p> <p>log.retention.hours = 8</p> </li> <li> <p>Press  to exit insert mode, then type :wq! to save and exit <li>Confirm if the file is created successfully, using below command:  <p>cat ~/cluster_config.txt</p> </li> <p></p>"},{"location":"day1/part1/2_cli/exercise/#step-3-create-the-configuration-object","title":"Step-3: Create the configuration object","text":"<p>Run the following command to push the configuration to the Amazon MSK service for use at cluster creation time:</p> <p>aws kafka create-configuration --name \"WorkshopMSKConfig\" --description \"Configuration used for MSK workshop - Auto topic creation; topic deletion; 8hrs retention\" --kafka-versions \"2.8.1\" \"2.7.0\" --server-properties fileb://cluster_config.txt</p> <p>The --kafka-versions option is used to tell Amazon MSK which versions of Amazon MSK this configuration is allowed to be used with. If you see an error like the following, then ensure that you've typed in the kafka version string correctly (including quotes):</p> <p>An error occurred (BadRequestException) when calling the CreateConfiguration operation: Unsupported KafkaVersion [2.7.1]. Valid values: [1.1.1, 2.1.0, 2.2.1, 2.3.1]</p> <p>When the command is run, it will return a JSON object, including the ARN for the configuration object. You should copy and paste this in to your text editor for use later, or assign it to an environment variable (export CLUSTER_ARN=\"arn:...\"). Example:</p> <p></p>"},{"location":"day1/part1/2_cli/exercise/#review-the-configuration-created","title":"Review the configuration created","text":"<p>You can review the configuration using the CLI. Using the ARN provided in the output Step above (or retrieved from aws kafka list-configurations) you can query for your saved configuration:</p> <p>aws kafka describe-configuration --arn $CLUSTER_ARN</p> <p>The output: </p> <p>For more details on creating and managing Amazon MSK Cluster Configuration, see the MSK Configuration Operations  document.</p>"},{"location":"day1/part1/2_cli/exercise/#step-4-create-the-cluster-definition-file","title":"Step-4: Create the cluster definition file","text":"<p>To complete this Step, you need the following:</p> <ol> <li>Private SubnetID us-east-1a (from step 1 above)</li> <li>Private SubnetID us-east-1b (from step 1 above)</li> <li>Private SubnetID us-east-1c (from step 1 above)</li> <li>Securitygroup ID for the SG \"MSKWorkshop-KafkaService\" (from Prerequisites lab)</li> <li>Cluster configuration ARN (from step 2 above)</li> </ol> <p>You will now combine the data above into a cluster definition file (clusterinfo.json). It will look something like this, where you will replace the values with the values from above:</p> <p>Example of a complete file:</p> <pre><code>{\n  \"BrokerNodeGroupInfo\": {\n     \"BrokerAZDistribution\": \"DEFAULT\",\n     \"InstanceType\": \"kafka.m5.large\",\n     \"ClientSubnets\": [\n         \"subnet-0777309bbfe01c152\", \"subnet-0d629834c7436ba42\", \"subnet-01236bbb66a65708d\"\n     ],\n     \"SecurityGroups\": [\n         \"sg-0b4e02d3f8487d13a\"\n     ],\n     \"StorageInfo\": {\n         \"EbsStorageInfo\": {\n            \"VolumeSize\": 100\n         }\n     }\n  },\n  \"ClusterName\": \"MSKWorkshopCluster-cli\",\n  \"ConfigurationInfo\": {\n     \"Arn\": \"arn:aws:kafka:us-east-1:&lt;aws-account-number&gt;:configuration/WorkshopMSKConfig/68151d80-d2fe-491b-8632-9e090cd7e2ea-21\",\n     \"Revision\": 1\n  },\n  \"EncryptionInfo\": {\n     \"EncryptionAtRest\": {\n         \"DataVolumeKMSKeyId\": \"\"\n     },\n     \"EncryptionInTransit\": {\n         \"InCluster\": true,\n         \"ClientBroker\": \"TLS_PLAINTEXT\"\n     }\n  },\n  \"EnhancedMonitoring\": \"PER_TOPIC_PER_BROKER\",\n  \"KafkaVersion\": \"2.8.1\",\n  \"NumberOfBrokerNodes\": 3,\n  \"OpenMonitoring\": {\n     \"Prometheus\": {\n         \"JmxExporter\": {\n            \"EnabledInBroker\": true\n         },\n         \"NodeExporter\": {\n            \"EnabledInBroker\": true\n         }\n     }\n  }\n}\n</code></pre>"},{"location":"day1/part1/2_cli/exercise/#step-5-create-the-cluster","title":"Step 5 - Create the cluster","text":"<p>We can now use the command line tool and the cluster definition to create the cluster:</p> <p>aws kafka create-cluster --cli-input-json file://~/clusterinfo.json </p> <p>The command will return a JSON object that containers your cluster ARN, name and state. Grab the ARN.</p> <p></p> <p>This step will take some time. You can move on to the next step to see how to monitor progress and review the cluster deployment.</p>"},{"location":"day1/part1/2_cli/exercise/#step-6-review-the-cluster-deployed","title":"Step 6 - Review the cluster deployed","text":"<p>You can check on your cluster configuration and status by using the cli and the --describe-cluster option. You will need the cluster arn for this, which you got from the last step.</p> <p>Use the ARN and get the cluster configuration and state, changing the example ARN to the one from the command above:</p> <p>aws kafka describe-cluster --cluster-arn arn:aws:kafka:us-east-1:&lt; AWS_Account_Number &gt;:cluster/MSKWorkshop/20a94343-552f-4298-9076-99673162e023-6 | grep -i state</p> <p></p> <p>You can alternatively check the same on the MSK Console (Right click -&gt; Open Link in New Tab).</p> <p></p> <p>When the cluster is ready, you will get the state as \"ACTIVE\".</p> <p></p> <p>You are done here! Proceed to Next Lab.</p>"},{"location":"day1/part1/Prerequisites/exercise/","title":"Prerequisites for running in AWS Event Account**","text":"<p>Please choose the region 'us-east-1'</p> <p></p>"},{"location":"day1/part1/Prerequisites/exercise/#preparation","title":"Preparation","text":"<p>The following steps will prepare you for the creation of the Amazon MSK cluster in this lab.</p> <p>Note that running this module will launch AWS resources that have an associated cost. If you are not running this lab as part of an Amazon MSK workshop using provided accounts, remember to clean up when you are done to keep from incurring ongoing charges for resources left running.</p>"},{"location":"day1/part1/Prerequisites/exercise/#get-the-client-security-group-from-cloudformation","title":"Get the Client Security Group from Cloudformation","text":"<p>By default, the cluster will be attached to the 'default' security group, which allows all ports between all members of the group. This is fine for testing, but it's not a best practice in production.</p> <p>We need two security groups - one to attach to producers, consumers, and admin hosts, and the other to attach to the Amazon MSK cluster that references the first.</p> <p>The CloudFormation template already created one of them - the Client security group. Look at the Outputs tab of the CloudFormation stack - msk-labs-default created and copy the value for the key KafkaClientEC2InstanceSecurityGroupId.</p> <p></p>"},{"location":"day1/part1/Prerequisites/exercise/#create-an-msk-security-group","title":"Create an MSK security group","text":"<ol> <li>Click on Services in the top left corner of the console, and select EC2</li> <li>Go to the EC2 - Security Groups Page (Right click -&gt; Open Link in New Tab).</li> <li>Click Create Security Group</li> </ol>"},{"location":"day1/part1/Prerequisites/exercise/#fill-out-the-form-as-follows","title":"Fill out the form as follows:","text":"<ol> <li>Security group name: MSKWorkshop-KafkaService</li> <li>Description: Access to the Kafka service on the MSK cluster</li> <li>VPC: [select the VPC you are using for your lab (MSKVPC)]</li> <li> <p>Create rules (Under the Inbound rules section. Use the below mentioned configuration)</p> </li> <li> <p>Click Create security group </p> </li> <li> <p>In the security group list, select the Group ID for the MSKWorkshop-KafkaService SG, and copy it to your notepad/text editor</p> </li> </ol> <p></p> <p>You are done here! Proceed to Launching MSK Cluster.</p>"},{"location":"day1/part1/Prerequisites/exercise/#a-click-add-rule-plaintext-kafka-broker-access","title":"(a). Click Add rule [Plaintext Kafka Broker Access]","text":"<ul> <li>Type: Custom TCP</li> <li>Protocol: TCP</li> <li>Port range: 9092</li> <li>Source: [paste the value of the KafkaClientEC2InstanceSecurityGroupId SG you copied in the previous step, from Cloudformation Outputs (msklab-KafkaClientInstance...)]</li> <li>Description: Plaintext Kafka</li> </ul>"},{"location":"day1/part1/Prerequisites/exercise/#b-click-add-rule-encrypted-kafka-broker-access","title":"(b). Click Add Rule [Encrypted Kafka Broker Access]","text":"<ul> <li>Type: Custom TCP</li> <li>Protocol: TCP</li> <li>Port range: 9094</li> <li>Source: [paste the value of the * KafkaClientEC2InstanceSecurityGroupId SG you copied in the previous step, from Cloudformation Outputs (msklab-KafkaClientInstance...)]</li> <li>Description: Encrypted Kafka</li> </ul>"},{"location":"day1/part1/Prerequisites/exercise/#c-click-add-rule-zookeeper-access","title":"(c). Click Add Rule [Zookeeper Access]","text":"<ul> <li>Type: Custom TCP</li> <li>Protocol: TCP</li> <li>Port range: 2181</li> <li>Source: [paste the value of the KafkaClientEC2InstanceSecurityGroupId SG you copied in the previous step, from Cloudformation Outputs (msklab-KafkaClientInstance...)]</li> <li>Description: Zookeeper access</li> </ul>"},{"location":"day1/part1/overview/exercise/","title":"Overview","text":"<p>This module will walk you through how to use the Console to create a custom configuration and an Amazon MSK Cluster.</p>"},{"location":"day1/part1/overview/exercise/#mandatory-step","title":"Mandatory step:","text":"<ol> <li> <p>Download the CloudFormation template</p> </li> <li> <p>Navigate to the CloudFormation Console  and deploy the CloudFormation using AWS Console by clicking Create stack on the top right and choose With new resources (standard) option</p> </li> <li> <p>Select Upload a template file and choose the mskid-full.yml file you downloaded and click Next </p> </li> <li>Provide Stack name: msk-labs-default and click Next </li> <li> <p>Scroll down completely and click Next</p> </li> <li> <p>Scroll down completely and Check box both options in Capabilities section </p> </li> <li> <p>click Submit</p> </li> </ol>"},{"location":"day1/part1/overview/exercise/#custom-configuration","title":"Custom Configuration","text":"<p>The custom configuration will enable us to provide a special configuration to the cluster. Review the available options to make sure you have what you need. For more information about configuration properties, see Apache Kafka Configuration.</p> <p>To learn how you can create a custom MSK configuration, list all configurations, or describe them, see Amazon MSK configuration operations. To create an MSK cluster with a custom MSK configuration, or to update a cluster with a new custom configuration, see Amazon MSK: How it works.</p> <p>When you update your existing MSK cluster with a custom MSK configuration, Amazon MSK does rolling restarts when necessary, and uses best practices to minimize customer downtime. For example, after Amazon MSK restarts each broker, Amazon MSK tries to let the broker catch up on data that the broker might have missed during the configuration update before it moves to the next broker.</p>"},{"location":"day1/part1/overview/exercise/#cluster","title":"Cluster","text":"<p>The cluster will be deployed into an existing VPC, with brokers deployed in 3 private subnets (one per AZ). We will use m5.large nodes for this exercise. If you are using an existing VPC, please ensure that there is a private subnet in each AZ into which you can deploy.</p> <p>Amazon MSK detects and automatically recovers from the most common failure scenarios for clusters so that your producer and consumer applications can continue their write and read operations with minimal impact. When Amazon MSK detects a broker failure, it mitigates the failure or replaces the unhealthy or unreachable broker with a new one. In addition, where possible, it reuses the storage from the older broker to reduce the data that Apache Kafka needs to replicate. Your availability impact is limited to the time required for Amazon MSK to complete the detection and recovery. After a recovery, your producer and consumer apps can continue to communicate with the same broker IP addresses that they used before the failure.</p>"},{"location":"day1/part1/overview/exercise/#architecture","title":"Architecture","text":"<p>The diagram demonstrates the interaction between the following components:</p>"},{"location":"day1/part1/overview/exercise/#broker-nodes","title":"Broker nodes","text":"<p>When creating an Amazon MSK cluster, you specify how many broker nodes you want Amazon MSK to create in each Availability Zone. In the example cluster shown in this diagram, there's one broker per Availability Zone. Each Availability Zone has its own virtual private cloud (VPC) subnet. Amazon MSK offers a 3 AZ or a 2 AZ Apache Kafka node layout. Currently, a 1 AZ (single AZ) Amazon MSK cluster is not available.</p>"},{"location":"day1/part1/overview/exercise/#zookeeper-nodes","title":"ZooKeeper nodes","text":"<p>Amazon MSK also creates the Apache ZooKeeper nodes for you. Apache ZooKeeper is an open-source server that enables highly reliable, distributed coordination. There is a dedicated ZooKeeper ensemble created for each Amazon MSK cluster that is fully obfuscated and included with each cluster at no additional cost.</p>"},{"location":"day1/part1/overview/exercise/#producers-consumers-and-topic-creators","title":"Producers, consumers, and topic creators","text":"<p>Amazon MSK lets you use Apache Kafka data-plane operations to create topics and to produce and consume data.**</p> <p>AWS CLI - You can use the AWS Command Line Interface (AWS CLI) or the APIs in the SDK to perform control-plane operations. For example, you can use the AWS CLI or the SDK to create or delete an Amazon MSK cluster, list all the clusters in an account, or view the properties of a cluster.</p>"},{"location":"day2/part4/1_Intro/exercise/","title":"Introduction","text":"<p>Serverless services allow us to build applications without having to worry about the underlying infrastructure. This allows developers to avoid provisioning, scaling, and managing resource utilization. In this workshop we will build a serverless data pipeline using Amazon MSK Serverless which enables data to be consumed from many different client applications to accomplish downstream tasks such as dashboards and analytics.</p> <p>Attendees will will learn how to:  1. Start a Serverless MSK Cluster  2. Deploy a Kafka Client Container based Lambda Function  3. Ingest data from various sources into a Kafka Client  4. Consume data for downstream tasks such as Quicksight Dashboards </p> <p></p> <p>This is a 300-level workshop, so some familiarly with using serverless services such as Lambda and Fargate ishelpful. No Apache Kafka experience is necessary. This workshop is intended to be finished in 2 hours.</p>"},{"location":"day2/part4/2_Setup/exercise/","title":"Setup MSK Serverless cluster","text":""},{"location":"day2/part4/2_Setup/exercise/#msk-serverless-overview","title":"MSK Serverless Overview","text":"<p>Apache Kafka is a distributed event store and stream-processing platform. Kafka provides a mechanism to decouple data processing from the source and target destinations as it's highly scalable and resilient. Kafka is very customizable, based on your workload, which introduces operational overhead.</p> <p>With serverless Kafka the tradeoff is you lose the flexibility of being able to configure the capacity of your cluster while gaining the ability to use Kafka through a single interface that just provides an endpoint for clients.</p> <p>In this module we will stand up a Serverless Apache Kafka Cluster by navigating through the AWS Console.</p>"},{"location":"day2/part4/2_Setup/exercise/#create-msk-serverless-cluster","title":"Create MSK Serverless Cluster","text":"<p>Navigate to Amazon MSK console page and click the Create cluster button on the top right.</p> <p></p>"},{"location":"day2/part4/2_Setup/exercise/#cluster-settings","title":"Cluster Settings","text":"<p>Choose Custom create to customize cluster networking and other settings. Provide the Cluster name.</p> <p></p> <p>Ensure cluster type is selected as Serverless and click Next button.</p> <p></p>"},{"location":"day2/part4/2_Setup/exercise/#networking","title":"Networking","text":"<p>In the VPC Configuration section, click Add subnet button to have cluster spread across 3 Availability Zones</p> <p></p> <p>Select MMVPC from the VPC dropdown. For Subnets, select the zones us-east-2a, us-east-2b, us-east-2c and choose Subnets with name starting PrivateSubnetMSK from dropdown for each zone.</p> <p></p> <p>Choose Custom security groups, remove the default security group and choose security group with name MSK Security Group from dropdown list and click Next button.</p> <p></p>"},{"location":"day2/part4/2_Setup/exercise/#security","title":"Security","text":"<p>MSK Serverless defaults to IAM role-based authentication. Click Next to proceed.</p> <p></p>"},{"location":"day2/part4/2_Setup/exercise/#metrics-and-tags","title":"Metrics and tags","text":"<p>Leave it to default and click Next to proceed.</p> <p></p>"},{"location":"day2/part4/2_Setup/exercise/#review-and-create","title":"Review and create","text":"<p>Review the configuration and click Create cluster button.</p> <p></p> <p>Wait for couple of minutes for cluster to be created and available to use. Wait for cluster status to change from Creating to  Active</p> <p> </p>"},{"location":"day2/part4/2_Setup/exercise/#copy-cluster-endpoint","title":"Copy Cluster Endpoint","text":"<p>Click on the View client information button and copy the details of cluster endpoint and save it on notepad for later use.</p> <p> </p>"},{"location":"day2/part4/2_Setup/exercise/#create-msk-topics","title":"Create MSK Topics","text":"<p>Now that the cluster is ready and available to use, we need to create Kafka topics to produce and consume data. Please follow instructions below to create Kafka topics needed for this workshop.</p>"},{"location":"day2/part4/2_Setup/exercise/#connect-to-kafka-client","title":"Connect to Kafka Client","text":"<p>Navigate to Amazon EC2 consolepage.</p> <p>On the EC2 home page click on Instances(running) link as show in below image.</p> <p></p> <p>On the EC2 instances page select the checkbox for the instance named KafkaClientInstance and click on Connect button on top right as shown in below image.</p> <p></p> <p>On the page Connect to instance, ensure you select the Session Manager tab and click on the Connect button.</p> <p></p> <p>This would open a new tab with EC2 terminal.</p>"},{"location":"day2/part4/2_Setup/exercise/#create-topics","title":"Create Topics","text":"<p>On the Terminal, please execute below command to change to ec2-user by running the below command:</p> <pre><code>sudo su - ec2-user\n</code></pre> <p></p> <p>Please execute below command to set your MSK cluster broker endpoint to the shell variable BS. Please replaceYOUR_CLUSTER_ENDPOINT in below command with the endpoint you copied in the last step of previous section as shown in the image.</p> <pre><code>export BS=YOUR_CLUSTER_ENDPOINT\n</code></pre> <p></p> <p>Execute command below to create the topics needed for workshop as shown in image</p> <pre><code>./create-topics.sh\n</code></pre> <p>You will see warnings printed to the terminal like below. You may ignore them.</p> <pre><code>SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n</code></pre> <p></p> <p>You should see the four MSK topics (clickstream, Departments_Agg, ClickEvents_UserId_Agg_Result, User_Sessions_Aggregates_With_Order_Checkout) created.</p> <p></p>"},{"location":"day2/part4/3_Producer/exercise/","title":"Produce clickstream data to MSK Serverless using Elastic Container Service","text":"<p>Now that we have successfully created topics in our cluster, our next step is to produce data to the cluster. In this step we will deploy a serverlessECS Fargate container that runs an application to produce sample clickstream data to MSK Serverless cluster.</p> <p>Navigate to Amazon ECS console page.</p> <p>On the left side menu click on Task Definitions to navigate to page that displays available Task Definitions.</p> <p> </p> <p>Select the checkbox against the available Task Definition and Click on Run Task option from Deploy menu.</p> <p></p>"},{"location":"day2/part4/3_Producer/exercise/#run-task","title":"Run Task","text":"<p>On the Run Task page, select Fargate for Launch type option and for theOperating system family select Linux from drop down. Refer to below image for more details on other fields</p> <p></p> <p>Expand the Networking section.</p> <p></p>"},{"location":"day2/part4/3_Producer/exercise/#vpc-and-security-groups","title":"VPC and security groups","text":"<p>Change VPC to MMVPC. Select PrivateSubnetMSKOne, PrivateSubnetMSKTwo and PrivateSubnetMSKThree.</p> <p>For Security Groups, select Use an existing security group, uncheck thedefault security group, and select the security group name starting with msk-serverless-workshop-cfn-ProducerECSTaskSecurity.</p> <p></p>"},{"location":"day2/part4/3_Producer/exercise/#advanced-options","title":"Advanced Options","text":"<p>Expand the section under Container Overrides.</p> <p>For BOOTSTRAP_STRING enter the value of your MSK Serverless cluster broker endpoint (you copied this earlier from the Amazon MSK cluster console page, with View client information).</p> <p></p> <p>Scroll to the bottom of the page and click on the Create button.</p> <p></p> <p>On the new page, wait for your task to get into Running status as shown in below image.</p> <p></p> <p>You have now successfully setup a producer ECS task that will continuously keep on producing clickstream data to Kafka topic.</p>"},{"location":"day2/part4/3_Producer/exercise/#check-schema-in-aws-glue-schema-registry","title":"Check schema in AWS Glue Schema Registry","text":"<p>Now that the producer ECS task is running, it would create the clickstream schema in AWS Glue Schema Registry.</p> <p>Navigate to Amazon Glue console page and select Stream schema registries from left menu.</p> <p>You would see the schema registry name serverless. Click on the registry name.</p> <p></p> <p>This would show the schema clickstream within the registry. Click on the schema name to see the schema versions. You would see the v1 over there.</p> <p></p> <p>Click on the version v1, this would show you the avro schema of the clickstream data produced by the ECS task.</p> <p></p>"},{"location":"day2/part4/4_LambdaConsumer/exercise/","title":"Consume data from MSK Serverless using Lambda","text":"<p>Now that we have setup our MSK Serverless cluster we want to store the datain Amazon S3. There are various options to do that, but in this lab, we will use AWS Lambda.</p> <p>The AWS Lambda function is already deployed for you in your AWS account. The only thing we need to setup is the Trigger. This Functions convert the records from Avro to CSV.</p> <p>Navigate to the AWS Lambda Console and choose click on the MskLambda-Function.</p> <p></p>"},{"location":"day2/part4/4_LambdaConsumer/exercise/#setup-amazon-msk-trigger","title":"Setup Amazon MSK Trigger","text":"<p>Click on the Add trigger button to create a new trigger for the Lambda function.</p> <p> </p> <p>Choose MSK as Source. Select your cluster in the drop down menu. Set the configuration parameters for the trigger:</p> <pre><code>- Batch Size: 20\n- Starting Position: Latest\n- Batch window: 20\n- Topic name: clickstream\n</code></pre> <p>After setting the configuration click Add to create the trigger.</p> <p></p> <p>Wait for the trigger to become active (this can take a few minutes).</p> <p></p> <p></p> <p>You have now successfully setup a Lambda functions that reads messages from a Kafka topic, transforms it into csv format and then stores it in Amazon S3.</p>"},{"location":"day2/part4/5_QS_Dashboard/exercise/","title":"Create Quicksight Dashboard","text":"<p>Now that we have transformed data, we can now use Amazon QuickSight to visualize and analyze data.</p>"},{"location":"day2/part4/5_QS_Dashboard/exercise/#create-a-quicksight-account","title":"Create a QuickSight Account","text":"<p>To begin head to the QuickSight page and create a new standard QuickSight account.</p> <p></p> <p></p> <p>Create a unique QuickSight account user name (e.g workshopuserRANDOM_NUMBERS) and you can use a fake email address for notifications.</p> <p></p> <p>You must also select your s3 bucket with the clickstream data</p> <p></p> <p></p> <p>Once s3 bucket selected, you can finish and create your QuickSight Account</p> <p></p> <p></p>"},{"location":"day2/part4/5_QS_Dashboard/exercise/#creating-a-dataset","title":"Creating a Dataset","text":"<p>Once your account is setup head over to the QuickSight Datasets Console. In the Dataset console click New dataset on the top right to create our dataset. Select S3 and enter a name for your dataset.</p> <p>Additionally, we have to create a manifest file to specify the exact data we want to utilize for our analysis. Open a text editor and create a JSON file (e.g. quicksight_manifest.json) and enter the following. Make sure to replace YOUR_DEST_BUCKET with your destination bucket.</p> <p>You may find the name of your destination bucket by navigating to the Amazon S3 Console Page. Select your bucket, copy the name from the bucket detail page, andpaste into your manifest file.</p> <p></p> <pre><code>{\n    \"fileLocations\": [\n        {\n            \"URIPrefixes\": [\n                \"s3://YOUR_DEST_BUCKET/\"\n            ]\n        }\n    ]\n}\n</code></pre> <p></p> <p>Click connect once file is uploaded, then press Visualize to get to the QuickSight analysis creator. You will be presented with the following pop-up. Select Interactive Sheet.</p> <p></p> <p></p>"},{"location":"day2/part4/5_QS_Dashboard/exercise/#playtime","title":"Playtime","text":"<p>Now its Playtime, what type of graphs can you create with the data provided?</p> <p>Can you create a pie chart of the data by device type? How about a bar chart highlighting records by event type? *What about a Metric that just has the total number of Events?</p> <p></p> <p>In the top right corner, you can publish your analysis as a Dashboard or export to a PDF. Feel free to check out what others around you have built.</p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/","title":"Consume using Kinesis Data Analytics and write to Amazon OpenSearch Service","text":"<p>In this module, we would like to consume data from MSK Serverless cluster using Amazon Kinesis Data Analytics - Flink, process the data and write the data in Amazon OpenSearch Service</p> <p>The OpenSearch Service is deployed in your AWS account and Dashboards are already configured. We need to configure Kinesis Data Analytics application with correct runtime parameters.</p> <p>Navigate to the AWS Kinesis Analytics Console and click on the application KDAFlinkClickstream-msk-serverless-workshop-cfn.</p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/#configure-kinesis-analytics-application","title":"Configure Kinesis Analytics Application","text":"<p>Click on the Configure button to update the Kinesis Data Analytics configuration.</p> <p></p> <p>Scroll to RunTime Properties section. Update BootStrapServers and keep restof the value as default.</p> <p></p> <p>Save your changes.</p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/#run-kinesis-analytics-application","title":"Run Kinesis Analytics Application","text":"<p>Click on the Run button to run the application. This takes a few minutes.</p> <p></p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/#open-apache-flink-dashboard","title":"Open Apache Flink Dashboard","text":"<p>Once the Kinesis Analytics application is running, click on Open Apache Flink dashboard to open the Flink dashboard.</p> <p></p> <p>Click on the Job Name Flink Streaming Job</p> <p></p> <p>This would show the Job details including the operator directed acyclic graph and data flowing through the operators.</p> <p></p> <p>You have now successfully setup a Kinesis Analytics application that reads messages from a Kafka topic, process the data and then write it to Amazon OpenSearch Service.</p> <p>Lets check the data in OpenSearch Dashboard!</p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/#amazon-opensearch-dashboard-visualisation","title":"Amazon OpenSearch Dashboard Visualisation","text":"<p>In this module, we would see the dashboard visualisation generated based onthe ingested data from Kinesis Analytics application.</p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/#amazon-opensearch-service","title":"Amazon OpenSearch Service","text":"<p>Go to AWS CloudFormation Console and click on stack named msk-serverless-workshop-cfn. Go to the Outputs tab of the stack.</p> <p></p> <p>Right-click on the OpenSearchDashboardEndpoint and click Open Link in new tab. This would try to redirect you to the OpenSearch Dashboard login page.</p> <p>As OpenSearch Service is deployed in a VPC, we are using NGinx reverse proxy to access the OpenSearch Dashboard outside of the VPC.</p> <pre><code>We are using self signed certificate for the Nginx. however recommended to use valid certificate for production.\n</code></pre> <p>If you are accessing the url using Google Chrome, you have to click on the webpage body and type thisisunsafe. This would redirect you to actual OpenSearch login page.</p> <p></p> <p>Go back to the CloudFormation output, retrieve the values for OpenSearchMasterUserName and OpenSearchMasterPassword and use it for login.</p> <pre><code>Ensure that there are no spaces copied for OpenSearchMasterPassword, else you won't be able to login to OpenSearch Dashboard.\n</code></pre> <p>Select Global tenant on the popup dialog box.</p>"},{"location":"day2/part4/6_KDA_Consumer/exercise/#amazon-opensearch-dashboard","title":"Amazon OpenSearch Dashboard","text":"<p>Click on the hamburger menu on the left of the screen as highlighted below and then click on Dashboard.</p> <p></p> <p>This would open the OpenSearch Dashboard where you can see the data gets plotted.</p> <p></p> <p>You have now confirmed data flowing to OpenSearch Service and visualizations are rendered.</p>"},{"location":"day2/part5/Prerequisites/exercise/","title":"2 - Prerequisites","text":""},{"location":"day2/part5/Prerequisites/exercise/#1-security-group-configuration","title":"1. Security Group Configuration","text":"<p>Add self route to MSK cluster SG</p> <ol> <li>Click on Services in the top left corner of the console, and select MSK</li> </ol> <p> 2. Go to the MSK (Right click -&gt; Open Link in New Tab) and choose the cluster MSKCluster-msk-labs-default  3. Click on Properties Tab and scroll down to Networking settings  4. In Networking settings, under Primary VPC configuration look for Security groups applied  5. Right click and open the Security Group in a New Tab  6. Copy the Security group name </p> <p> 7. Scroll down and you will find Inbound rules, then click Edit Inbound rules</p> <p> 8. Scroll down and Add rule: (Add a self route as below)</p> <ul> <li>Type: All TCP</li> <li>Protocol: TCP</li> <li>Port range: 0 - 65535</li> <li>Source: [paste the value of the Security Group Id you copied in the step-6]</li> <li>Description: Plaintext Kafka - Lambda </li> </ul> <p> 9. Click Save rules</p>"},{"location":"day2/part5/Prerequisites/exercise/#2-create-s3-bucket","title":"2. Create S3 Bucket","text":"<p>This S3 bucket will be used to store athena query information and spill data information from Lambda connector setup.</p> <ol> <li>Navigate to S3 Console</li> <li>Click Create bucket</li> </ol> <p>  3. In General configuration, provide the Bucket name as  msk-athena-bucket-&lt; AWS-Account-Number &gt;  4. Scroll down completely and click Create bucket.  Leave all other configuration options as default</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#3-configure-athena","title":"3. Configure Athena","text":"<p>This is used to configure Athena result location.</p> <ol> <li>Navigate to Athena Console and click on Launch query editor</li> </ol> <p> 2. Click on Settings tab</p> <p> 3. In Query result and encryption settings, click on Manage  4. In Query result location and encryption, click on Browse S3</p> <p> 5. Choose the radio button beside the bucket msk-athena-bucket-&lt; AWS Account Number &gt;</p> <p> 6. Click Choose 7. Add the folder path to the Location of query result as below:</p> <ul> <li>athena-spill/</li> <li>Click Save</li> </ul> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#4-setting-up-iam","title":"4. Setting up IAM","text":"<p>This is used to create IAM policies and IAM role for the redshift cluster</p>"},{"location":"day2/part5/Prerequisites/exercise/#create-iam-policy","title":"Create IAM Policy","text":"<p>Create an IAM Policy, which provides permission for communication with the Amazon MSK cluster. The policy you need depends on the authentication method used on your cluster, if you use Amazon MSK. See Authentication and Authorization for Apache Kafka APIs for authentication methods available in Amazon MSK.</p> <ol> <li>Navigate to IAM Console, Click on left pane and choose Policies in Acess management section</li> </ol> <p> 2. Click on Create policy</p> <p> 3. In Policy editor, choose JSON button</p> <p> 4. Copy and paste the below policy in the Policy editor section ( Overwrite the default provided policy )</p> <p>An IAM policy for Amazon MSK using unauthenticated access:</p> <pre><code>    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"VisualEditor0\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"kafka:GetBootstrapBrokers\"\n                ],\n                \"Resource\": \"*\"\n            }\n        ]\n    }\n</code></pre> <p> 5. Click Next</p> <p> 6. In Policy details, provide below details</p> <ul> <li>Policy name: MSK-unauthenticated-access</li> <li>Description: An IAM policy for Amazon MSK using unauthenticated access</li> </ul> <p> 7. Click Create policy</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#create-iam-role","title":"Create IAM Role","text":"<p>Create an IAM role with a trust policy that allows your Amazon Redshift cluster to assume the role. For information about how to configure the trust policy for the IAM role, see Authorizing Amazon Redshift to access other AWS services on your behalf. </p> <ol> <li>Navigate to IAM Console, Click on left pane and choose Roles in Acess management section</li> </ol> <p> 2. Click on Create role</p> <p> 3. In Use cases for other AWS services, choose Redshift from the drop down  4. Choose Redshift - Customizable and click Next</p> <p> 5. Select the above create policy (MSK-unauthenticated-access) and Click Next</p> <p> 6. In Role details section, provide below details:</p> <p>Role name: MSK-Redshift-role</p> <p> 7. Click Create role</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#5-create-redshift-subnet-group","title":"5. Create Redshift Subnet Group","text":"<ol> <li>Navigate to Redshift Console, Click on left pane and choose Subnet groups in Configurations section</li> </ol> <p>  2. Click Create cluster subnet group</p> <p> 3. In Cluster subnet group details, provide below configuration details:</p> <ul> <li>Name: msk-redshift-vpc-subnet-group</li> <li>Description: This subnet group contains msk and redshift subnet groups </li> </ul> <p> 4. In Add subnets section, go to VPC drop down and choose MSKVPC</p> <p> 5. Click on Add all the subnets for this VPC</p> <p> 6. Click Create cluster subnet group</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#6-create-redshift-cluster","title":"6. Create Redshift Cluster","text":"<ol> <li>Navigate to Redshift Console, Click on left pane and choose Provisioned clusters dashboard and click on Create cluster</li> </ol> <p>  2. In Cluster configuration, provide the below details:</p> <pre><code>    Cluster identifier              : redshift-cluster-1\nChoose the size of the cluster  : I'll choose\nNode type                       : dc2.large\nNumber of nodes                 : 1\n</code></pre> <p> 3. In Database configurations, provide the below details:</p> <pre><code>    Admin user name     : awsuser\n    Admin user password : &lt; Password &gt;\n</code></pre> <p> 4. In Associated IAM roles, click on Associate IAM roles</p> <p> 5. Select the radio button beside the IAM role msk-redshift-role and click Associate IAM roles</p> <p> 6. In Additional configurations, toggle the Use defaults option.</p> <p> 7. Expand Network and security section and provide below configuration:</p> <pre><code>    Virtual private cloud (VPC) : MSKVPC\nVPC security groups         : msk-labs-default-MSKSecurityGroup\nCluster subnet group        : msk-redshift-vpc-subnet-group\nAvailability Zone           : us-east-2a\nEnhanced VPC routing        : Turn off\nPublicly accessible         : Check Box (True)\nElastic IP address          : None\n</code></pre> <p>  8. Click Create cluster</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#7-cloud-9-setup","title":"7. Cloud 9 Setup","text":"<p>Open Cloud9 , select the host msk-labs-default-Cloud9EC2Bastion, open Cloud9 IDE and install following tools:</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#install-kafka-tools","title":"Install kafka tools","text":"<p>In this step, you install Apache Kafka client libraries and tools on the cloud9 setup:</p> <ul> <li>wget https://archive.apache.org/dist/kafka/2.6.2/kafka_2.12-2.6.2.tgz</li> <li>tar -xzf kafka_2.12-2.6.2.tgz</li> <li>ln -s kafka_2.12-2.6.2 kafka</li> </ul> <p> </p>"},{"location":"day2/part5/Prerequisites/exercise/#install-jq-envsubst","title":"Install jq, envsubst","text":"<p>sudo yum -y install jq gettext bash-completion moreutils</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#8-kafka-setup","title":"8. Kafka Setup","text":""},{"location":"day2/part5/Prerequisites/exercise/#create-topic","title":"Create Topic","text":"<ol> <li>Go to the MSK (Right click -&gt; Open Link in New Tab) and choose the cluster MSKCluster-msk-labs-default</li> </ol> <p> 2. In Cluster summary, Click on View client information</p> <p> 3. Copy any one of the broker-ids(connection string for the private endpoint) from Authentication type - Plaintext</p> <p>Note: You will get three endpoints for each of the brokers. You only need one broker endpoint for the following step.</p> <p> 4. Go to the EC2 console and open aws-cloud9 instance properties by clicking on it.Copy the security group id and save it. </p> <p> 5. Go back to the MSK console. Select Properties scroll down, and under Networking settings click on the security group. This will open the security group.</p> <p> 6. From the security group, click on Edit inbound rules.</p> <p> 7. Click on Add rule with the following configuration. </p> <ul> <li>Type: All TCP</li> <li>Protocol: TCP</li> <li>Port range: 0-65535</li> <li>Source type: Custom</li> <li>Source: Paste the security group that you copied from the EC2 console</li> </ul> <p> 8. Now we will be creating a sample topic using cloud9 host.  9. Open Cloud9 , Click open beside the host msk-labs-default-Cloud9EC2Bastion  10. Click on icon (+), choose New Terminal [Please check the below screenshot]  11. In the above terminal, first we are going to create a MSK Topic. </p> <p>To create a Kafka Topic:</p> <p>Run the following command, replacing BootstrapServerString with one of the broker endpoints that you obtained in the previous step.</p> <p>path-to-your-kafka-installation/bin/kafka-topics.sh --create --bootstrap-server BootstrapServerString --replication-factor 3 --partitions 1 --topic MSKTopicName</p> <p>Example command:</p> <p>kafka_2.12-2.6.2/bin/kafka-topics.sh --create --bootstrap-server b-1.mskclustermsklabsdefa.1je9hf.c21.kafka.us-east-1.amazonaws.com:9092 --replication-factor 3 --partitions 1 --topic orders</p> <p>If the command succeeds, you see the following message: Created topic orders.</p> <p></p>"},{"location":"day2/part5/Prerequisites/exercise/#produce-and-consume-data","title":"Produce and consume data","text":"<p>To produce and consume messages, in the above terminal itself we will be running the following command to start a console producer.</p> <p>To produce data to a Kafka Topic: Replace BootstrapServerString with the plaintext connection string that you obtained in create topic</p> <p>path-to-your-kafka-installation/bin/kafka-console-producer.sh --broker-list BootstrapServerString --producer.config client.properties --topic MSKTopicName</p> <p>Example command:</p> <p>kafka_2.12-2.6.2/bin/kafka-console-producer.sh --broker-list b-1.mskclustermsklabsdefa.1je9hf.c21.kafka.us-east-1.amazonaws.com:9092 --topic orders</p> <p></p> <p>Enter the following data, and press Enter. </p>"},{"location":"day2/part5/Prerequisites/exercise/#data","title":"Data:","text":"<ul> <li>{\"customer_id\": \"000\",\"item_id\":1}</li> <li>{\"customer_id\": \"abe\",\"item_id\":2}</li> <li>{\"customer_id\": \"def\",\"item_id\":3}</li> <li>{\"customer_id\": \"ghi\",\"item_id\":4}</li> <li>{\"customer_id\": \"jkl\",\"item_id\":5}</li> <li>{\"customer_id\": \"mno\",\"item_id\":6}</li> <li>{\"customer_id\": \"pqr\",\"item_id\":7}</li> <li>{\"customer_id\": \"rst\",\"item_id\":8}</li> <li>{\"customer_id\": \"uvw\",\"item_id\":9}</li> <li>{\"customer_id\": \"xyz\",\"item_id\":10}</li> <li>{\"customer_id\": \"000\",\"item_id\":11}    </li> <li>{\"customer_id\": \"abe\",\"item_id\":12}  </li> <li>{\"customer_id\": \"000\",\"item_id\":13}</li> </ul> <p>Note: Every time you enter a line and press Enter, that line is sent to your Apache Kafka cluster as a separate message.</p>"},{"location":"day2/part5/Prerequisites/exercise/#to-consume-data-from-a-kafka-topic","title":"To consume data from a Kafka Topic","text":"<p>In the below terminal, we will be running the following command to start a console consumer.</p> <p>Replace BootstrapServerString with the plaintext connection string that you saved earlier.</p> <p>path-to-your-kafka-installation/bin/kafka-console-consumer.sh --bootstrap-server BootstrapServerString --topic MSKTopicName --from-beginning</p> <p>Example command:</p> <p>kafka_2.12-2.6.2/bin/kafka-console-consumer.sh --bootstrap-server b-1.mskclustermsklabsdefa.1je9hf.c21.kafka.us-east-1.amazonaws.com:9092 --topic orders --from-beginning</p> <p></p> <p>You start seeing the messages you entered earlier when you used the console producer command.</p>"},{"location":"day2/part5/athena/exercise/","title":"MSK Integration with Athena","text":"<p>In this lab, we will analyze real-time streaming data in Amazon MSK with Amazon Athena.</p>"},{"location":"day2/part5/athena/exercise/#solution-overview","title":"Solution overview","text":"<p>First, we will show you how to get started with real-time SQL analytics using Athena and its connector for MSK. The process involves:</p> <ol> <li> <p>Registering the schema of your streaming data with AWS Glue Schema Registry. Schema Registry is a feature of AWS Glue that allows you to validate and reliably evolve streaming data against JSON schemas. It can also serialize data into a compressed format, which helps you save on data transfer and storage costs.</p> </li> <li> <p>Creating a new instance of the Amazon Athena MSK Connector. Athena connectors are pre-built applications that run as serverless AWS Lambda applications, so there\u2019s no need for standalone data export processes.</p> </li> <li> <p>Using the Athena console to run interactive SQL queries on your Kafka topics.</p> </li> </ol>"},{"location":"day2/part5/athena/exercise/#get-started-with-athenas-connector-for-amazon-msk","title":"Get started with Athena\u2019s connector for Amazon MSK","text":"<p>In this section, we\u2019ll cover the steps necessary to set up your MSK cluster to work with Athena to run SQL queries on your Kafka topics.</p>"},{"location":"day2/part5/athena/exercise/#1-define-the-schema-of-your-kafka-topics-with-aws-glue-schema-registry","title":"1. Define the schema of your Kafka topics with AWS Glue Schema Registry","text":"<p>To run SQL queries on your Kafka topics, you\u2019ll first need to define the schema of your topics as Athena uses this metadata for query planning. AWS Glue makes it easy to do this with its Schema Registry feature for streaming data sources.</p> <p></p>"},{"location":"day2/part5/athena/exercise/#creating-glue-stream-schema-registries","title":"Creating Glue Stream schema registries","text":"<ol> <li>In Stream schema registries, click on Add registry</li> </ol> <p> 2. Provide the below configuration:</p> <ul> <li>Name: customer_schema</li> <li>Description: {AthenaFederationMSK}</li> </ul> <p>3.Click Add registry</p> <p></p> <p>Schema Registry allows you to centrally discover, control, and evolve streaming data schemas for use in analytics applications such as Athena. With AWS Glue Schema Registry, you can manage and enforce schemas on your data streaming applications using convenient integrations with Apache Kafka. To learn more, see AWS Glue Schema Registry and Getting started with Schema Registry.</p> <p>If configured to do so, the producer of data can auto-register its schema and changes to it with AWS Glue. This is especially useful in use cases where the contents of the data is likely to change over time. However, you can also specify the schema manually, resembling the following JSON structure.</p>"},{"location":"day2/part5/athena/exercise/#creating-schema-in-glue-stream-schema-registries","title":"Creating Schema in Glue Stream schema registries","text":"<ol> <li>Choose the Stream schehma registry : msk-glue-registry</li> <li>Click Create schema</li> </ol> <p> 3. Provide the below configuration</p> <ul> <li>Schema name: orders</li> <li>Registry: customer_schema</li> <li>Data format: JSON</li> <li>Compatibility mode: Full - combination of 'Backward' and 'Forward'</li> <li>Description: {AthenaFederationMSK}</li> </ul> <p></p> <pre><code>##### First schema version\n\n{\n\"topicName\": \"orders\",\n\"message\": {\n\"dataFormat\": \"json\",\n\"fields\": [\n{\n\"name\": \"customer_id\",\n\"mapping\": \"customer_id\",\n\"type\": \"VARCHAR\"\n},\n{\n\"name\": \"item_id\",\n\"mapping\": \"item_id\",\n\"type\": \"INTEGER\"\n}\n]\n}\n}\n</code></pre> <p>  4. Click Create schema and version</p> <p>For additional information on schema set up, see Schema examples for the AWS Glue Schema Registry.</p>"},{"location":"day2/part5/athena/exercise/#2-configure-the-athena-connector-for-msk","title":"2. Configure the Athena connector for MSK","text":"<p>With your schema registered with Glue, the next step is to set up the Athena connector for MSK. We recommend using the Athena console for this step. For more background on the steps involved, see Deploying a connector and connecting to a data source.</p> <p>In Athena, federated data source connectors are applications that run on AWS Lambda and handle communication between your target data source and Athena. When a query runs on a federated source, Athena calls the Lambda function and tasks it with running the parts of your query that are specific to that source. To learn more about the query execution workflow, see Using Amazon Athena Federated Query in the Amazon Athena User Guide.</p> <ol> <li>Navigate to Athena console and selecting Data sources on the left navigation, then choose Create data source</li> </ol> <p>  2. Click on Create Data sources</p> <p> 3. Choose a data source: Type MSK</p> <p> 4. Choose the radio button beside the connector Amazon MSK and click Next  5. Enter the following details in Data source details section:</p> <ul> <li>Data source name: msk</li> <li>Description - To connect to amazon msk cluster</li> </ul> <p> 6. Under Connection details section, select Create Lambda function. This will bring you to the AWS Lambda console where you\u2019ll provide additional configuration properties</p> <p> 7. Scroll down to Application settings and provide below details </p> <p></p> <pre><code>##### Fetch MSK Configuration\n</code></pre> <ul> <li>To get some of the details you will have to navigate to MSK console and click on cluster name as MSKCluster-msk-labs-default</li> <li>In Cluster summary, Click on View client Information</li> </ul> <p></p> <ul> <li>Copy any one of the broker-ids(connection string for the private endpoint) from Authentication type - Plaintext and paste it in the Lambda configuration \"KafkaEndpoint\" <p>Note: You will get three endpoints for each of the brokers. You only need one broker endpoint for the following step. (Paste this in KafkaEndpoint) and click Done</p> </li> </ul> <p></p> <ul> <li>Click on Properties Tab and scroll down to Networking settings</li> </ul> <p></p> <ul> <li>In Networking settings, under Primary VPC configuration look for Subnets and Security groups applied.</li> </ul> <p></p> <ul> <li>Copy security-group-id and one subnet-id, paste them in the below configuration accordingly</li> </ul> <pre><code>    KafkaEndpoint: &lt;paste the broker-id here&gt;\n    LambdaFunctionName: msk-athena-lambda-function\n    SecurityGroupIds: &lt;paste the security-group-id here&gt;\n    SpillBucket: msk-athena-bucket-&lt;paste the AWS-Account-Number&gt;\n    SpillPrefix: msk-athena-spill\n    SubnetIds: &lt;paste the subnet-id here &gt;\n</code></pre> <p></p> <p>  8. Click on I acknowledge that this app creates custom IAM roles and resource policies.  9. Click Deploy (wait for a minute to auto-refresh the page)</p> <p> 10. Close the Lambda page and go back to previous tab (Athena Console - Enter data source details)  11. In Connection details, use the refresh button and choose Lambda function which we created in the previous step ( msk-athena-lambda-function ) </p> <p></p> <p> 12. Click Next  13. Review the details and click Create data source</p> <p></p>"},{"location":"day2/part5/athena/exercise/#3-run-queries-on-streaming-data-using-athena","title":"3. Run queries on streaming data using Athena","text":"<p>With your MSK data connector set up, you can now run SQL queries on the data. Let\u2019s explore a few use cases in more detail.</p> <p>Use case: interactive analysis If you want to run queries that aggregate, group, or filter your MSK data, you can run interactive queries using Athena. These queries will run against the current state of your Kafka topics at the time the query was submitted.</p> <ol> <li>Navigate to Athena Console</li> <li>Under Editor tab, in Data section, you will observe the following:</li> </ol> <p> 3. Choose msk from the drop down.  4. Choose customer_schema from the database</p> <p></p> <p>Before running any queries, it may be helpful to validate the schema and data types available within your Kafka topics. To do this, run the DESCRIBE command on your Kafka topic, which appears in Athena as a table, as shown below. In this query, the orders table corresponds to the topic you specified in the Schema Registry customer_schema.</p> <p>DESCRIBE msk.customer_schema.orders</p> <p></p> <p>Now that you know the contents of your topic, you can begin to develop analytical queries. A sample query for querying all data in a topic is:</p> <pre><code>select * from msk.customer_schema.orders;\n</code></pre> <p></p> <p>You can proceed to next lab!</p>"},{"location":"day2/part5/overview/exercise/","title":"MSK Integration with Athena and Redshift","text":"<p>Integrating MSK with Athena and Redshift helps in building real-time analytics solution on streaming data, enabling timely insights and data-driven decision-making. </p> <p>Amazon Athena provides serverless interactive queries on data stored in Amazon S3. By integrating MSK with Athena, you can directly query the data in  yout Kafka topics, without the need for additional data movement or transformations. This simplifies the data processing pipeline and reduces latency in accessing and analyzing the data.  Leran more about Amazon Athena MSK Connector.</p> <p>Amazon Redshift is a fully managed data warehousing service that offers high-performance analysis of large-scale data sets. By integrating MSK with Redshift, you can stream data from Kafka into Redshift for near-real-time analysis. This allows you to leverage Redshift's advanced anlaytics capabilities and perform complex queries on the streaming data.  Learn more about Integrating MSK with Redshift.</p>"},{"location":"day2/part5/redshift/exercise/","title":"MSK Integration with Redshift","text":"<p>In this lab, we will analyze real-time streaming data in Amazon MSK with Amazon Redshift.</p>"},{"location":"day2/part5/redshift/exercise/#solution-overview","title":"Solution overview","text":"<p>The purpose of Amazon Redshift streaming ingestion is to simplify the process for directly ingesting stream data from a streaming service into Amazon Redshift. This works with Amazon MSK and Amazon MSK Serverless. Amazon Redshift streaming ingestion removes the need to stage an Amazon MSK topic in Amazon S3 before ingesting the stream data into Amazon Redshift.</p> <p>On a technical level, streaming ingestion, from Amazon Managed Streaming for Apache Kafka, provides low-latency, high-speed ingestion of stream or topic data into an Amazon Redshift materialized view. Following setup, using materialized view refresh, you can take in large data volumes.</p> <p>Set up Amazon Redshift streaming ingestion for Amazon MSK by performing the following steps:</p> <ul> <li>Create an external schema that maps to the streaming data source.</li> <li>Create a materialized view that references the external schema.</li> </ul> <p>Streaming ingestion and Amazon Redshift Serverless - The configuration steps in this topic apply both to provisioned Amazon Redshift clusters and to Amazon Redshift Serverless. For more information, see Streaming ingestion considerations.</p>"},{"location":"day2/part5/redshift/exercise/#fetching-iam-role-info","title":"Fetching IAM Role info","text":"<ol> <li>Navigate to Redshift console</li> </ol> <p> 2. Scroll down to Associated IAM roles and click on the role msk-redshift-role</p> <p> 3. Copy the ARN from the Summary section. (Paste it in a note-pad)</p> <p></p>"},{"location":"day2/part5/redshift/exercise/#fetching-msk-cluster-info","title":"Fetching MSK cluster info","text":"<ol> <li>Navigate to MSK Console</li> <li>Click on the Cluster name - MSKCluster-msk-labs-default</li> <li>Copy the ARN from the Cluster summary section. (Paste it in a note-pad)</li> </ol>"},{"location":"day2/part5/redshift/exercise/#create-an-external-schema","title":"Create an external schema","text":"<ol> <li>Navigate to Redshift query editor v2</li> </ol> <p>  2. In the left pane, choose the redshift cluster: redshift-cluster-1 3. Once it is explanded, go to editor section and create an external schema to map to the Amazon MSK cluster.</p> <pre><code>CREATE EXTERNAL SCHEMA MySchema\nFROM MSK\nIAM_ROLE { default | 'iam-role-arn' }\nAUTHENTICATION { none | iam }\nCLUSTER_ARN 'msk-cluster-arn';\n</code></pre> <pre><code>In the FROM clause, Amazon MSK denotes that the schema maps data from Managed Kafka Services.\n\nStreaming ingestion for Amazon MSK provides the following authentication types, when you create the external schema:\n\nnone \u2013 Specifies that there is no authentication step.\n\niam \u2013 Specifies IAM authentication. When you choose this, make sure that the IAM role has permissions for IAM authentication.\n\nAdditional Amazon MSK authentication methods, such as TLS authentication or a username and password, aren't supported for streaming ingestion.\n\nCLUSTER_ARN specifies the Amazon MSK cluster that you\u2019re streaming from.\n</code></pre> <p>Final Command:</p> <pre><code>CREATE EXTERNAL SCHEMA msk_schema\nFROM MSK\nIAM_ROLE 'arn:aws:iam::&lt; AWS_Account_Number &gt;:role/msk-redshift-role'\nAUTHENTICATION none CLUSTER_ARN 'arn:aws:kafka:us-east-1:&lt; AWS_Account_Number &gt;:cluster/MSKCluster-msk-labs-default/0ae312ea-a86b-44dd-xxxxxxxxxxxxxxxx';\n</code></pre> <p> Once you execute this command you should get Returned rows as 0 ( RequestID should appear in green)</p>"},{"location":"day2/part5/redshift/exercise/#create-a-materialized-view","title":"Create a Materialized view","text":"<ol> <li>Navigate to Redshift query editor v2</li> <li>In the left pane, choose the redshift cluster: redshift-cluster-1</li> <li> <p>Once it is explanded, go to editor section and create a Materialized view to map topic to a queryable MV.</p> <p>Materialized view command:</p> <pre><code>CREATE MATERIALIZED VIEW msk_topic_orders AUTO REFRESH YES AS\nSELECT \"kafka_partition\", \"kafka_offset\", \"kafka_timestamp_type\", \"kafka_timestamp\", \"kafka_key\", JSON_PARSE(\"kafka_value\") as Data, \"kafka_headers\"\nFROM msk_schema.orders;\n</code></pre> <p>-&gt; Once you execute this command you should get Returned rows as 0 ( RequestID should appear in green)</p> </li> </ol> <p> 4. Refresh the view, which invokes Amazon Redshift to read from the topic and load data into the materialized view.</p> <p><code>REFRESH MATERIALIZED VIEW msk_topic_orders;</code>  5. Query data in the materialized view:</p> <pre><code>    select data from msk_topic_orders;\n</code></pre> <p>-&gt; The materialized view is updated directly from the topic when REFRESH is run. You create a materialized view that maps to the Kafka topic data source. You can perform filtering and aggregations on the data as part of the materialized view definition. Your streaming ingestion materialized view (base materialized view) can reference only one Kafka topic, but you can create additional materialized views that join with the base materialized view and with other materialized views or tables.        </p> <p>You have reached the end of workshop!</p>"}]}